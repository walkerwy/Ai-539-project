{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Running:\n",
    "Please Install all from the requirements.txt (pip install -r requirements.txt).   \n",
    "Download the pre-trained SwinT weights from here https://drive.google.com/drive/folders/1HBw5NGGw8DjkyNurksCP5v8a5f0FG7zU and put them in this folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder_decoder_layers = 3\n",
    "encoder_decoder_heads = 8\n",
    "embedded_dim_size = 512\n",
    "max_length = 32\n",
    "coco_dataset_ratio = 50\n",
    "coco_dataset_dir = \"./coco\"\n",
    "vocab_size = 30522\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "patience = 3\n",
    "weight_decay = 1e-5\n",
    "att_feats_dim = 2048\n",
    "encoder_model = \"microsoft/swin-base-patch4-window7-224-in22k\"\n",
    "decoder_model = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Format datasets\n",
    "This will take some time to finishing running the first time. It took me roughly 40 minutes.\n",
    "\n",
    "This section does the following actions:\n",
    "1. Downloads the Dataset\n",
    "2. Keeps images with only 3 or 4 dim\n",
    "3. Transforms the dataset \n",
    "4. Turns the data set into data loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programming\\Python3-12-3\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for HuggingFaceM4/COCO contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/COCO\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTImageProcessor, GPT2TokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Download the train, val and test splits of the COCO dataset\n",
    "train_ds = load_dataset(\"HuggingFaceM4/COCO\", split=f\"train[:{coco_dataset_ratio}%]\", cache_dir=coco_dataset_dir)\n",
    "valid_ds = load_dataset(\"HuggingFaceM4/COCO\", split=f\"validation[:{coco_dataset_ratio}%]\", cache_dir=coco_dataset_dir)\n",
    "test_ds = load_dataset(\"HuggingFaceM4/COCO\", split=\"test\", cache_dir=coco_dataset_dir)\n",
    "\n",
    "\n",
    "# Filter all non 3 or 4 dim images out\n",
    "# Can change num_proc, but might be errors with np\n",
    "train_ds = train_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=1)\n",
    "valid_ds = valid_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=1)\n",
    "test_ds = test_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=1)\n",
    "\n",
    "\n",
    "# Does pre processing on the data set\n",
    "# This includes pre-trained ViTimage feature extraction and tokenizing captions\n",
    "# I am unsure if the paper does any of this pre processing\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(decoder_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "image_processor = ViTImageProcessor.from_pretrained(encoder_model)\n",
    "\n",
    "def preprocess(items):\n",
    "    pixel_values = image_processor(items[\"image\"], return_tensors=\"pt\").pixel_values.to(device)\n",
    "    targets = tokenizer([sentence[\"raw\"] for sentence in items[\"sentences\"]],\n",
    "                        max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "    return {'pixel_values': pixel_values, 'labels': targets[\"input_ids\"]}\n",
    "\n",
    "train_dataset = train_ds.with_transform(preprocess)\n",
    "valid_dataset = valid_ds.with_transform(preprocess)\n",
    "test_dataset = test_ds.with_transform(preprocess)\n",
    "\n",
    "\n",
    "# Turns the dataset into a torch DataLoader\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.stack([x['labels'] for x in batch])\n",
    "    }\n",
    "\n",
    "train_dataset_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset_loader = DataLoader(valid_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)\n",
    "test_dataset_loader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "Creates the PureT model from the paper\n",
    "\n",
    "This section does the following actions:\n",
    "1. Creates the SWIN Transformer used by PureT\n",
    "2. Creates the PureT encoder\n",
    "3. Creates the PureT decoder\n",
    "4. Creates the PureT model\n",
    "\n",
    "Download the pre-trained SwinT weights from here https://drive.google.com/drive/folders/1HBw5NGGw8DjkyNurksCP5v8a5f0FG7zU and put them in this folder before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, einsum\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "import math\n",
    "from functools import reduce\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torch.autograd import Variable\n",
    "\n",
    "##############################\n",
    "#\n",
    "# SWIN Transformer code\n",
    "# from https://github.com/232525/PureT/blob/5581b5d10ae3bb9f9c859f6644e90db8beaf992b/models/backbone/swin_transformer_backbone.py#L458\n",
    "#\n",
    "##############################\n",
    "def window_partition (x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "# Finds shifted windows\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Window attention layer\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 fused_window_process=False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    r\"\"\" Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                               use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        # self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        # self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    \"\"\"\n",
    "    # forward\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \"\"\"\n",
    "    \n",
    "    # forward w/o head\n",
    "    def forward(self, x):\n",
    "        # extract image features，[B, L, D]\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        return x\n",
    "    \n",
    "    def load_weights(self, pretrained_model):\n",
    "        checkpoint = torch.load(pretrained_model, map_location='cpu')\n",
    "        # print(checkpoint['patch_embed.proj.bias'])\n",
    "        self.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "\n",
    "##############################\n",
    "#\n",
    "# PureT Encoder code\n",
    "# from https://github.com/232525/PureT/blob/5581b5d10ae3bb9f9c859f6644e90db8beaf992b/models/encoder_decoder/PureT_encoder.py\n",
    "#\n",
    "##############################\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_embed_dim, relu_dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_embed_dim)\n",
    "        self.act = nn.ReLU()    # ReLU / GELU / CELU\n",
    "        self.fc2 = nn.Linear(ffn_embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(relu_dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WindowAttentionEncoder(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim=512, window_size=(12, 12), num_heads=8, \n",
    "                 nW=4, ind_gx=True):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.nW = nW\n",
    "        self.ind_gx = ind_gx # 是否在注意力机制内部单独计算全局特征\n",
    "\n",
    "        # 相对位置编码，用于grid特征的每一个window\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # [2*Wh-1 * 2*Ww-1, nH]\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        \n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        \n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.o_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        # N = Ww * Wh      仅window区域的grid特征，Ww和Wh为window_size\n",
    "        # N = Ww * Wh + 1  window区域的grid特征加图像全局特征\n",
    "        B_, N, C = x.size()\n",
    "        \"\"\"\n",
    "        print('*'*30)\n",
    "        print('raw gx', x[:, -1, :].min(), x[:, -1, :].max(), x[:, -1, :].mean())\n",
    "        print('raw all', x.min(), x.max(), x.mean())\n",
    "        # \"\"\"\n",
    "        \n",
    "        # [B*nW, nH, N, C//nH]，其中nW为window数量，nH为num_heads\n",
    "        q = self.q_linear(x).view(B_, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_linear(x).view(B_, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_linear(x).view(B_, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        \"\"\"\n",
    "        print('gx q', q[:, :, -1, :].min(), q[:, :, -1, :].max(), q[:, :, -1, :].mean())\n",
    "        print('gx k', k[:, :, -1, :].min(), k[:, :, -1, :].max(), k[:, :, -1, :].mean())\n",
    "        print('gx v', v[:, :, -1, :].min(), v[:, :, -1, :].max(), v[:, :, -1, :].mean())\n",
    "        print('all q', q.min(), q.max(), q.mean())\n",
    "        print('all k', k.min(), k.max(), k.mean())\n",
    "        print('all v', v.min(), v.max(), v.mean())\n",
    "        # \"\"\"\n",
    "        \n",
    "        # [B*nW, nH, N, N]，其中nW为window数量，nH为num_heads\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        # print(attn.min(), attn.max(), attn.mean())\n",
    "\n",
    "        # 相对位置编码，仅window区域内的grid特征之间计算\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # [Wh*Ww, Wh*Ww, nH]\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # [nH, Wh*Ww, Wh*Ww]\n",
    "        # 如果加入了全局特征，相对位置编码与全局特征无关\n",
    "        if N == self.window_size[0] * self.window_size[1]:\n",
    "            attn = attn + relative_position_bias.unsqueeze(0)\n",
    "        else:\n",
    "            # 仅对window区域的grid特征部分嵌入相对位置编码\n",
    "            attn[:, :, :-1, :-1] = attn[:, :, :-1, :-1] + relative_position_bias.unsqueeze(0)\n",
    "        \n",
    "        \"\"\"\n",
    "        print(relative_position_bias.min(), relative_position_bias.max(), relative_position_bias.mean())\n",
    "        print(attn.min(), attn.max(), attn.mean())\n",
    "        # \"\"\"\n",
    "                    \n",
    "        # 此处mask用于区分SW-MSA/W-MSA\n",
    "        # mask: [nW, N, N]，\n",
    "        # 其中nW为window数量，N=Ww*Wh or Ww*Wh+1，Ww和Wh为window_size\n",
    "        if mask is not None:\n",
    "            # mask = mask.masked_fill(mask == float(-100), float(-1e9))\n",
    "            nW = mask.shape[0]\n",
    "            # attn: [B*nW, nH, N, N] --> [B, nW, nH, N, N]\n",
    "            # mask: [nW, N, N]       --> [1, nW,  1, N, N]\n",
    "            # print('IN', attn.view(B_ // nW, nW, self.num_heads, N, N)[0, 2, 0, 0, :])\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            # [B*nW, nH, N, N]\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "        else:\n",
    "            attn = attn\n",
    "        \n",
    "        # TODO：代码精简\n",
    "        # 单独处理全局特征，从attn中分离出全局特征权重，单独加权计算\n",
    "        # [B*nW, nH, N]\n",
    "        if self.ind_gx and N != self.window_size[0] * self.window_size[1]:\n",
    "            # [B*nW, nH, N] -> [B, nW, nH, N] -> [B, nH, nW, N]\n",
    "            gx_attn = attn[:, :, -1, :]\n",
    "            gx_attn = gx_attn.view(B_ // self.nW, self.nW, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "            # [B, nH, nW, N-1] --> [B, nH, nW * (N-1)] 即 [B, nH, H * W]\n",
    "            gx_attn_1 = gx_attn[:, :, :, :-1].contiguous().view(B_ // self.nW, self.num_heads, -1)\n",
    "            # [B, nH, nW, 1] --> [B, nH, 1]\n",
    "            gx_attn_2 = gx_attn[:, :, :, -1:].mean(-2)\n",
    "            # [B, nH, nW * (N-1) + 1] 即 [B, nH, H * W + 1]\n",
    "            gx_attn = torch.cat([gx_attn_1, gx_attn_2], -1)\n",
    "            # 全局特征权重 [B, nH, nW * (N-1) + 1] 即 [B, nH, H * W + 1]\n",
    "            gx_attn = self.softmax(gx_attn)\n",
    "            \"\"\"\n",
    "            if mask is None:\n",
    "                # [B, 8, 145]\n",
    "                # print(gx_attn.size())\n",
    "                # print(gx_attn[0, 0, :-1].view(12, 12))\n",
    "                print(gx_attn[:12, :, :].max(-1))\n",
    "                if gx_attn[:, :, -1].max() > 0.001:\n",
    "                    print('>>> gx alpha:', gx_attn[:, :, -1].max())\n",
    "            # \"\"\"\n",
    "            # [B, nH, nW * (N-1)]\n",
    "            gx_attn_1 = gx_attn[:, :, :-1] \n",
    "            # [B, nH, nW * (N-1)] --> [B, nH, nW, N-1] --> [B, nW, nH, N-1] --> [B*nW, nH, N-1]\n",
    "            gx_attn_1 = gx_attn_1.view(B_ // self.nW, self.num_heads, self.nW , -1).permute(0, 2, 1, 3).contiguous().view(B_, self.num_heads, -1)\n",
    "            # [B, nH, 1]\n",
    "            gx_attn_2 = gx_attn[:, :, -1:] \n",
    "            # [B, nH, 1] --> [B, nH, nW, 1] --> [B, nW, nH, 1] --> [B*nW, nH, 1]\n",
    "            gx_attn_2 = gx_attn_2.unsqueeze(-1).repeat(1, 1, self.nW, 1).permute(0, 2, 1, 3).contiguous().view(B_, self.num_heads, -1)\n",
    "            # [B*nW, nH, N] --> [B*nW, nH, 1, N]\n",
    "            gx_attn = torch.cat([gx_attn_1, gx_attn_2], -1).unsqueeze(-2)\n",
    "            gx = (gx_attn @ v).transpose(1, 2).reshape(B_, C)\n",
    "            # print(gx.size())\n",
    "            gx = gx.view(B_ // self.nW, self.nW, -1).sum(1)\n",
    "            # print(gx.size())\n",
    "            gx = gx.unsqueeze(1).repeat(1, self.nW, 1).view(B_, C)\n",
    "            # print(gx.size())\n",
    "            \n",
    "        # softmax计算权重\n",
    "        attn = self.softmax(attn)\n",
    "        \"\"\"\n",
    "        # [B*nW, 8, Ww*Wh, Ww*Wh]\n",
    "        # print(attn.size())\n",
    "        if attn[:12, :, :-1, -1].max() > 0.1:\n",
    "            print(attn.size())\n",
    "            print(attn[:12, :, 0, :].max(-1))\n",
    "            # print(attn[:12, :, :-1, -1].max())\n",
    "            # print(attn[:12, :, :-1, -1].max(-1))\n",
    "            # print(attn[:8, :, :-1, -1].argmax(-1, keepdim=True))\n",
    "        # \"\"\"\n",
    "        \n",
    "        # 加权求和，[B*nW, N, C]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        # 替换掉x中全局特征，[B*nW, C]\n",
    "        if self.ind_gx and N != self.window_size[0] * self.window_size[1]:\n",
    "            x[:, -1, :] = gx\n",
    "        x = self.o_linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embed_dim=512, \n",
    "        input_resolution=(12, 12), \n",
    "        num_heads=8, \n",
    "        window_size=12,    # 窗口大小，如果窗口大小和输入一致，则退化为普通MSA\n",
    "        shift_size=0,      # shift大小，0 OR window_size // 2\n",
    "        mlp_ratio=4,       # FeedForward 中间层维度变换\n",
    "        dropout=0.1,\n",
    "        use_gx=False\n",
    "    ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.embed_dim = embed_dim                            # 1536\n",
    "        self.input_resolution = input_resolution  # (12， 12)\n",
    "        self.num_heads = num_heads                # 8\n",
    "        self.window_size = window_size            # 12 / 6\n",
    "        self.shift_size = shift_size   # shift_size可用于区分SW-MSA / W-MSA\n",
    "        self.mlp_ratio = mlp_ratio     # 4\n",
    "        self.use_gx = use_gx           # False\n",
    "        self.nW = (input_resolution[0] // window_size)**2\n",
    "        \n",
    "        # if window size is larger than input resolution, \n",
    "        # we don't partition windows\n",
    "        # 且window_size需要能够被input resolution整除，才能正确划分窗口\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        # 构造注意力核心操作层\n",
    "        self.encoder_attn = WindowAttentionEncoder(\n",
    "            embed_dim=embed_dim, \n",
    "            window_size=to_2tuple(self.window_size), \n",
    "            num_heads=num_heads,\n",
    "            nW = self.nW\n",
    "        )\n",
    "        # dropout同时用于encoder_attn和ff_layer输出\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # 构造FeedForward层\n",
    "        ffn_embed_dim = int(embed_dim * mlp_ratio)\n",
    "        self.ff_layer = FeedForward(\n",
    "            embed_dim = embed_dim, \n",
    "            ffn_embed_dim = ffn_embed_dim, \n",
    "            relu_dropout = dropout\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # 此处mask为SW-MSA使用\n",
    "        # [nW, w_s * w_s, w_s * w_s]\n",
    "        # nW 为 window 数量，w_s 为 window_size\n",
    "        # [4, 36, 36] 当input_resolution=(12, 12)，window_size=6时\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # [1, H, W, 1]\n",
    "            # 对 [H, W] 大小进行分区\n",
    "            # 分区的目的在于，shift之后，进行window划分时，一个window内包含多个区域，可能彼此不相临，需要进行标号区分\n",
    "            # 数字相同表示在shift之前区域相邻\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            \n",
    "            \"\"\"\n",
    "            # 也可以按如下分区\n",
    "            # 数字相同表示在shift之前区域相邻\n",
    "            h_slices = (slice(0, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            \"\"\"\n",
    "            \n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-1e9)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "    \n",
    "    def forward(self, x, att_mask=None):\n",
    "        # x: query / key / value  [B, L, C] 其中，L = H * W\n",
    "        # x为grid特征，一个batch内每个样本特征数量一致，注意力计算时无需mask标注\n",
    "        # att_mask 为 None 即可，不参与计算\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        short_cut = x\n",
    "        \n",
    "        # 如果使用全局特征，需要划分出全局特征和grid特征\n",
    "        if self.use_gx:\n",
    "            assert L == (H * W +1), \"input feature has wrong size\"\n",
    "            gx = x[:, -1, :]   # [B, C]\n",
    "            x  = x[:, :-1, :]  # [B, H * W, C]\n",
    "        else:\n",
    "            assert L == H * W, \"input feature has wrong size\"\n",
    "            gx = None\n",
    "            x = x\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # 循环移位，SW-MSA核心操作，W-MSA时不做处理\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # windows划分，比如 12x12 的区域被划分为4个 6 x 6 的windows\n",
    "        # [B, H, W, C] --> [nW*B, window_size * window_size, C]\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "        \n",
    "        # 如果使用全局特征，需要嵌入 gx 到 x_windows 中，\n",
    "        # 每个 window 内部的注意力机制加入了全局特征\n",
    "        # 全局特征被复制了 nW 次，即窗口个数\n",
    "        if self.use_gx:\n",
    "            # [B, C] -> [B, 1, C] -> [B, nW, C] -> [B*nW, C]\n",
    "            gx_ = gx.unsqueeze(1).repeat(1, x_windows.size()[0]//gx.size()[0], 1).view(x_windows.size()[0], -1)\n",
    "            x_windows = torch.cat([x_windows, gx_.unsqueeze(1)], 1) # [B*nW, window_size*window_size + 1, C]\n",
    "            \n",
    "            # 对SW-MSA需要的mask进行扩充\n",
    "            # 使用 torch.nn.functional.pad 填充\n",
    "            if self.attn_mask is None:\n",
    "                _mask = self.attn_mask\n",
    "            else:\n",
    "                _mask = F.pad(\n",
    "                    self.attn_mask, \n",
    "                    pad=(0, 1, 0, 1, 0, 0),\n",
    "                    mode='constant', \n",
    "                    value=0.0\n",
    "                ) # [nW, window_size*window_size + 1, window_size*window_size + 1]\n",
    "        else:\n",
    "            _mask = self.attn_mask\n",
    "            \n",
    "        # W-MSA/SW-MSA\n",
    "        # x_windows: [B*nW, Ww*Wh+1, C] \n",
    "        # mask: [nW, Ww*Wh+1, Ww*Wh+1,] or None\n",
    "        attn_windows = self.encoder_attn(x_windows, mask=_mask)  # nW*B, window_size*window_size, C\n",
    "        \n",
    "        # 如果使用全局特征，需要从注意力机制输出（attn_windows）中拆分出 gx\n",
    "        if self.use_gx:\n",
    "            # 此处gx计算为4个windows内部的_gx的均值\n",
    "            # 也可以在注意力机制内部进行处理（由注意力层的ind_gx参数控制\n",
    "            # [B*nW, C] --> [B, nW, C] --> [B, C]\n",
    "            gx = attn_windows[:, -1, :].view(-1, self.nW, 512).mean(1)\n",
    "            # [B*nW, Ww*Wh, C]\n",
    "            attn_windows = attn_windows[:, :-1, :]\n",
    "        \n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "        \n",
    "        # 如果使用全局特征，需要再次合并gird特征和全局特征\n",
    "        if self.use_gx:\n",
    "            # [B, H*W+1, C]\n",
    "            x = torch.cat([x, gx.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        # 注意力后的残差\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm1(x + short_cut)\n",
    "        \n",
    "        # FeedForward及残差\n",
    "        short_cut = x\n",
    "        x = self.ff_layer(x)\n",
    "        # dropout 残差 LayerNorm在此加入\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm2(x + short_cut)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embed_dim=512, \n",
    "        input_resolution=(12, 12), \n",
    "        depth=3, \n",
    "        num_heads=8, \n",
    "        window_size=12,  # =12 退化为普通MSA结构\n",
    "        shift_size=6,    # =0  无SW-MSA，仅W-MSA\n",
    "        mlp_ratio=4,\n",
    "        dropout=0.1,\n",
    "        use_gx=False\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_gx = use_gx\n",
    "        \n",
    "        # 构建 W-MSA / SW-MSA 层\n",
    "        # 输入特征尺寸为 144 = 12 x 12，如果构建 SW-MSA 层，\n",
    "        # 则需要将 window_size 设置得更小，比如设置为 6，且shift_size > 0\n",
    "        # SW-MSA仅在偶数层被构造，W-MSA在奇数层构造\n",
    "        # 如：W-MSA，SW-MSA，W-MSA，SW-MSA ......\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                embed_dim=embed_dim, \n",
    "                input_resolution=input_resolution,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else shift_size,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                dropout=dropout,\n",
    "                use_gx=use_gx\n",
    "            ) for i in range(self.depth)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, att_mask=None):\n",
    "        # x: [B, H*W, C]\n",
    "        # 对于grid特征，att mask为None亦可\n",
    "        # 全局特征初始化，图像特征均值 [B, C]\n",
    "        if att_mask is not None:\n",
    "            gx = (torch.sum(x * att_mask.unsqueeze(-1), 1) / torch.sum(att_mask.unsqueeze(-1), 1))\n",
    "        else:\n",
    "            gx = x.mean(1)\n",
    "        \n",
    "        # 如果使用全局特征，则需要将全局特征gx和grid特征x合并送入后续层处理\n",
    "        if self.use_gx:\n",
    "            O = torch.cat([x, gx.unsqueeze(1)], dim=1)  # [B, H*W+1, C]\n",
    "        else:\n",
    "            O = x\n",
    "            \n",
    "        # 核心操作层\n",
    "        for layer in self.layers:\n",
    "            O = layer(O, att_mask)\n",
    "        \n",
    "        if self.use_gx:\n",
    "            gx = O[:, -1, :]\n",
    "            x  = O[:, :-1, :]\n",
    "        else:\n",
    "            gx = O.mean(1)\n",
    "            x = O\n",
    "        return gx, x\n",
    "\n",
    "\n",
    "##############################\n",
    "#\n",
    "# PureT Decoder code\n",
    "# from https://github.com/232525/PureT/blob/5581b5d10ae3bb9f9c859f6644e90db8beaf992b/models/encoder_decoder/PureT_decoder.py\n",
    "#\n",
    "##############################\n",
    "def position_embedding(input, d_model):\n",
    "    input = input.view(-1, 1)\n",
    "    dim = torch.arange(d_model // 2, dtype=torch.float32, device=input.device).view(1, -1)\n",
    "    sin = torch.sin(input / 10000 ** (2 * dim / d_model))\n",
    "    cos = torch.cos(input / 10000 ** (2 * dim / d_model))\n",
    "\n",
    "    out = torch.zeros((input.shape[0], d_model), device=input.device)\n",
    "    out[:, ::2] = sin\n",
    "    out[:, 1::2] = cos\n",
    "    return out\n",
    "\n",
    "\n",
    "def sinusoid_encoding_table(max_len, d_model, padding_idx=None):\n",
    "    pos = torch.arange(max_len, dtype=torch.float32)\n",
    "    out = position_embedding(pos, d_model)\n",
    "\n",
    "    if padding_idx is not None:\n",
    "        out[padding_idx] = 0\n",
    "    return out\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.o_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        \n",
    "        self.clear_buffer()\n",
    "        \n",
    "    def init_buffer(self, batch_size):\n",
    "        # [B, nH, 0, C/nH]\n",
    "        self.buffer_key = torch.zeros((batch_size, self.num_heads, 0, self.head_dim), device='cuda')\n",
    "        self.buffer_value = torch.zeros((batch_size, self.num_heads, 0, self.head_dim), device='cuda')\n",
    "        \n",
    "    def clear_buffer(self):\n",
    "        self.buffer_key = None\n",
    "        self.buffer_value = None\n",
    "        \n",
    "    def apply_to_states(self, fn):\n",
    "        self.buffer_key = fn(self.buffer_key)\n",
    "        self.buffer_value = fn(self.buffer_value)\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        \"\"\"\n",
    "        Decoder部分有两部分进行注意力：\n",
    "            1）单词嵌入自注意力，q/k/v大小均为[B, L, D]\n",
    "            2）单词嵌入与图像特征（包含全局特征）的cross attention，q的大小为[B, L, D]\n",
    "               k/v的大小为[B, M+1, D]\n",
    "        输出的维度大小只与q的维度大小相关\n",
    "        \"\"\"\n",
    "        B_, N, C = q.size()\n",
    "        # 线性变换\n",
    "        q = self.q_linear(q).view(B_, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(B_, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(B_, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 存储buffer，用于inference时单词嵌入自注意力\n",
    "        if self.buffer_key is not None and self.buffer_value is not None:\n",
    "            self.buffer_key = torch.cat([self.buffer_key, k], dim=2)\n",
    "            self.buffer_value = torch.cat([self.buffer_value, v], dim=2)\n",
    "            k = self.buffer_key\n",
    "            v = self.buffer_value\n",
    "            \n",
    "        # 注意力核心操作\n",
    "        # [B, nH, L, L] or [B, nH, L, M+1]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "            \n",
    "        out = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        out = self.o_linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_heads=8, dropout=0.1, ff_dropout=0.1, use_gx=False):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.word_attn = MultiHeadSelfAttention(\n",
    "            embed_dim = embed_dim, \n",
    "            num_heads = num_heads\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.cross_att = MultiHeadSelfAttention(\n",
    "            embed_dim = embed_dim, \n",
    "            num_heads = num_heads\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ff_layer = FeedForward(\n",
    "            embed_dim = embed_dim, \n",
    "            ffn_embed_dim = embed_dim * 4, \n",
    "            relu_dropout = ff_dropout\n",
    "        )\n",
    "        self.layer_norm3 = torch.nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.use_gx = use_gx\n",
    "        if self.use_gx:\n",
    "            # 方式2，concat接Linear / Linear+GLU\n",
    "            self.fuse_layer = nn.Sequential(\n",
    "                nn.Linear(embed_dim*2, embed_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            )\n",
    "            self.fuse_layer_norm = nn.LayerNorm(embed_dim)\n",
    "\t    \n",
    "\n",
    "    def apply_to_states(self, fn):\n",
    "        self.word_attn.apply_to_states(fn)\n",
    "\n",
    "    def init_buffer(self, batch_size):\n",
    "        self.word_attn.init_buffer(batch_size)\n",
    "\n",
    "    def clear_buffer(self):\n",
    "        self.word_attn.clear_buffer()\n",
    "\n",
    "    def precompute(self, encoder_out):\n",
    "        # key, value2 = self.cross_att.precompute(encoder_out, encoder_out)\n",
    "        # return key, value2\n",
    "        pass\n",
    "\n",
    "    def forward(self, gx, x, encoder_out, seq_mask, att_mask=None):\n",
    "        # 单词嵌入自注意力\n",
    "        # short_cut = x\n",
    "        # 在单词嵌入自注意力阶段，嵌入图像的全局特征\n",
    "        # 方式2:concat接Linear+GLU / Linear\n",
    "        if self.use_gx:\n",
    "            x_cat = torch.cat([x, gx.unsqueeze(1).expand_as(x)], dim=-1)\n",
    "            x = self.fuse_layer(x_cat) + x\n",
    "            x = self.fuse_layer_norm(x)\n",
    "        short_cut = x\n",
    "        \n",
    "        x = self.word_attn(\n",
    "            q = x,\n",
    "            k = x,\n",
    "            v = x,\n",
    "            mask = seq_mask\n",
    "        )\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm1(x + short_cut)\n",
    "\n",
    "        # 单词嵌入与图像特征（可包含全局特征）cross 注意力\n",
    "        short_cut = x\n",
    "        if self.use_gx:\n",
    "            kv = torch.cat([encoder_out, gx.unsqueeze(1)], 1)\n",
    "            if att_mask is not None:\n",
    "                # [B, 1, M+1]，对于grid特征，直接设置为None亦可\n",
    "                _att_mask = torch.cat(\n",
    "                    [att_mask, torch.ones(att_mask.size(0), device='cuda').unsqueeze(1).unsqueeze(1)], 2\n",
    "                ).long()\n",
    "            else:\n",
    "                _att_mask = None\n",
    "        else:\n",
    "            kv = encoder_out\n",
    "            _att_mask = att_mask\n",
    "            \n",
    "        x = self.cross_att(\n",
    "            q = x,\n",
    "            k = kv,\n",
    "            v = kv,\n",
    "            mask = _att_mask,\n",
    "            # precompute=False\n",
    "        )\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm2(x + short_cut)\n",
    "        \n",
    "        # Feedforward\n",
    "        short_cut = x\n",
    "        x = self.ff_layer(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm3(x + short_cut)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embed_dim=512, \n",
    "        depth=3,\n",
    "        num_heads=8,\n",
    "        dropout=0.1, \n",
    "        ff_dropout=0.1, \n",
    "        use_gx=False\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.embed_dim = embed_dim\n",
    "        self.use_gx = use_gx\n",
    "        for i in range(depth):\n",
    "            sublayer = DecoderLayer( \n",
    "                embed_dim = embed_dim, \n",
    "                num_heads = num_heads, \n",
    "                dropout = dropout, \n",
    "                ff_dropout = ff_dropout,\n",
    "                use_gx = use_gx\n",
    "            )\n",
    "            self.layers.append(sublayer)\n",
    "            \n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "        \n",
    "        self.word_embed = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.embed_scale = math.sqrt(self.embed_dim)\n",
    "        self.pos_embed = nn.Embedding.from_pretrained(\n",
    "            sinusoid_encoding_table(100, self.embed_dim, 0), freeze=True\n",
    "        )\n",
    "        \n",
    "        self.generator = nn.Linear(self.embed_dim, self.vocab_size, bias=True)\n",
    "                \n",
    "        self.clear_buffer()\n",
    "\n",
    "    def init_buffer(self, batch_size):\n",
    "        self.seq_len = 0\n",
    "        for layer in self.layers:\n",
    "            layer.init_buffer(batch_size)\n",
    "\n",
    "    def clear_buffer(self):\n",
    "        self.seq_len = None\n",
    "        for layer in self.layers:\n",
    "            layer.clear_buffer()\n",
    "\n",
    "    def apply_to_states(self, fn):\n",
    "        for layer in self.layers:\n",
    "            layer.apply_to_states(fn)\n",
    "\n",
    "    def precompute(self, encoder_out):\n",
    "        p_att_feats = []\n",
    "        for layer in self.layers:\n",
    "            key, value2 = layer.precompute(encoder_out)\n",
    "            p_att_feats.append((key, value2))\n",
    "        return p_att_feats\n",
    "\n",
    "    def forward(self, gx, seq, encoder_out, seq_mask=None, att_mask=None):\n",
    "        if att_mask is not None:\n",
    "            att_mask = att_mask.unsqueeze(1)  # [B, 1, M]\n",
    "        \n",
    "        seq_len = seq.size()[1]\n",
    "        pos_indx = torch.arange(1, seq_len + 1, device='cuda').view(1, -1)\n",
    "        if self.seq_len is not None:\n",
    "            seq_len = self.seq_len + seq_len\n",
    "            self.seq_len = seq_len\n",
    "            pos_indx = torch.arange(seq_len, seq_len + 1, device='cuda').view(1, -1)\n",
    "            \n",
    "        # 词汇嵌入 + 位置嵌入\n",
    "        # [B, seq_len, C] for training or [B, 1, C] for inference\n",
    "        x = self.embed_scale * self.word_embed(seq) + self.pos_embed(pos_indx)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(gx, x, encoder_out, seq_mask, att_mask)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        out = self.generator(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "#\n",
    "# PureT model code\n",
    "# from  https://github.com/232525/PureT/blob/5581b5d10ae3bb9f9c859f6644e90db8beaf992b/models/basic_model.py\n",
    "# and   https://github.com/232525/PureT/blob/5581b5d10ae3bb9f9c859f6644e90db8beaf992b/models/pure_transformer.py\n",
    "#\n",
    "##############################\n",
    "class BasicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicModel, self).__init__()\n",
    "\n",
    "    def select(self, batch_size, beam_size, t, candidate_logprob):\n",
    "        selected_logprob, selected_idx = torch.sort(candidate_logprob.view(batch_size, -1), -1, descending=True)\n",
    "        selected_logprob, selected_idx = selected_logprob[:, :beam_size], selected_idx[:, :beam_size]\n",
    "        return selected_idx, selected_logprob\n",
    "\n",
    "    def beam_search(self, init_state, init_logprobs, **kwargs):\n",
    "        # function computes the similarity score to be augmented\n",
    "        def add_diversity(beam_seq_table, logprobsf, t, divm, diversity_lambda, bdash):\n",
    "            local_time = t - divm\n",
    "            unaug_logprobsf = logprobsf.clone()\n",
    "            for prev_choice in range(divm):\n",
    "                prev_decisions = beam_seq_table[prev_choice][local_time]\n",
    "                for sub_beam in range(bdash):\n",
    "                    for prev_labels in range(bdash):\n",
    "                        logprobsf[sub_beam][prev_decisions[prev_labels]] = logprobsf[sub_beam][prev_decisions[prev_labels]] - diversity_lambda\n",
    "            return unaug_logprobsf\n",
    "\n",
    "        def beam_step(logprobsf, unaug_logprobsf, beam_size, t, beam_seq, beam_seq_logprobs, beam_logprobs_sum, state):\n",
    "            #INPUTS:\n",
    "            #logprobsf: probabilities augmented after diversity\n",
    "            #beam_size: obvious\n",
    "            #t        : time instant\n",
    "            #beam_seq : tensor contanining the beams\n",
    "            #beam_seq_logprobs: tensor contanining the beam logprobs\n",
    "            #beam_logprobs_sum: tensor contanining joint logprobs\n",
    "            #OUPUTS:\n",
    "            #beam_seq : tensor containing the word indices of the decoded captions\n",
    "            #beam_seq_logprobs : log-probability of each decision made, same size as beam_seq\n",
    "            #beam_logprobs_sum : joint log-probability of each beam\n",
    "\n",
    "            ys,ix = torch.sort(logprobsf,1,True)\n",
    "            candidates = []\n",
    "            cols = min(beam_size, ys.size(1))\n",
    "            rows = beam_size\n",
    "            if t == 0:\n",
    "                rows = 1\n",
    "            for c in range(cols): # for each column (word, essentially)\n",
    "                for q in range(rows): # for each beam expansion\n",
    "                    #compute logprob of expanding beam q with word in (sorted) position c\n",
    "                    local_logprob = ys[q,c].item()\n",
    "                    candidate_logprob = beam_logprobs_sum[q] + local_logprob\n",
    "                    local_unaug_logprob = unaug_logprobsf[q,ix[q,c]]\n",
    "                    candidates.append({'c':ix[q,c], 'q':q, 'p':candidate_logprob, 'r':local_unaug_logprob})\n",
    "            candidates = sorted(candidates,  key=lambda x: -x['p'])\n",
    "            \n",
    "            new_state = [_.clone() for _ in state]\n",
    "            #beam_seq_prev, beam_seq_logprobs_prev\n",
    "            if t >= 1:\n",
    "            #we''ll need these as reference when we fork beams around\n",
    "                beam_seq_prev = beam_seq[:t].clone()\n",
    "                beam_seq_logprobs_prev = beam_seq_logprobs[:t].clone()\n",
    "            for vix in range(beam_size):\n",
    "                v = candidates[vix]\n",
    "                #fork beam index q into index vix\n",
    "                if t >= 1:\n",
    "                    beam_seq[:t, vix] = beam_seq_prev[:, v['q']]\n",
    "                    beam_seq_logprobs[:t, vix] = beam_seq_logprobs_prev[:, v['q']]\n",
    "                #rearrange recurrent states\n",
    "                for state_ix in range(len(new_state)):\n",
    "                #  copy over state in previous beam q to new beam at vix\n",
    "                    new_state[state_ix][:, vix] = state[state_ix][:, v['q']] # dimension one is time step\n",
    "                #append new end terminal at the end of this beam\n",
    "                beam_seq[t, vix] = v['c'] # c'th word is the continuation\n",
    "                beam_seq_logprobs[t, vix] = v['r'] # the raw logprob here\n",
    "                beam_logprobs_sum[vix] = v['p'] # the new (sum) logprob along this beam\n",
    "            state = new_state\n",
    "            return beam_seq,beam_seq_logprobs,beam_logprobs_sum,state,candidates\n",
    "\n",
    "        beam_size = kwargs['BEAM_SIZE']\n",
    "        group_size = 1 #kwargs['GROUP_SIZE']\n",
    "        diversity_lambda = 0.5 #kwargs['DIVERSITY_LAMBDA']\n",
    "        constraint = False #kwargs['CONSTRAINT']\n",
    "        max_ppl = False #kwargs['MAX_PPL']\n",
    "        bdash = beam_size // group_size\n",
    "\n",
    "        beam_seq_table = [torch.LongTensor(max_length, bdash).zero_() for _ in range(group_size)]\n",
    "        beam_seq_logprobs_table = [torch.FloatTensor(max_length, bdash).zero_() for _ in range(group_size)]\n",
    "        beam_logprobs_sum_table = [torch.zeros(bdash) for _ in range(group_size)]\n",
    "\n",
    "        # logprobs # logprobs predicted in last time step, shape (beam_size, vocab_size+1)\n",
    "        done_beams_table = [[] for _ in range(group_size)]\n",
    "        state_table = [list(torch.unbind(_)) for _ in torch.stack(init_state).chunk(group_size, 2)]\n",
    "        logprobs_table = list(init_logprobs.chunk(group_size, 0))\n",
    "        # END INIT\n",
    "\n",
    "        for t in range(max_length + group_size - 1):\n",
    "            for divm in range(group_size): \n",
    "                if t >= divm and t <= max_length + divm - 1:\n",
    "                    # add diversity\n",
    "                    logprobsf = logprobs_table[divm].data.float()\n",
    "                    # suppress previous word\n",
    "                    if constraint and t-divm > 0:\n",
    "                        logprobsf.scatter_(1, beam_seq_table[divm][t-divm-1].unsqueeze(1).cuda(), float('-inf'))\n",
    "                    # suppress UNK tokens in the decoding\n",
    "                    logprobsf[:,logprobsf.size(1)-1] -= 1000  \n",
    "                    # diversity is added here\n",
    "                    # the function directly modifies the logprobsf values and hence, we need to return\n",
    "                    # the unaugmented ones for sorting the candidates in the end. # for historical\n",
    "                    # reasons :-)\n",
    "                    unaug_logprobsf = add_diversity(beam_seq_table,logprobsf,t,divm,diversity_lambda,bdash)\n",
    "\n",
    "                    # infer new beams\n",
    "                    beam_seq_table[divm],\\\n",
    "                    beam_seq_logprobs_table[divm],\\\n",
    "                    beam_logprobs_sum_table[divm],\\\n",
    "                    state_table[divm],\\\n",
    "                    candidates_divm = beam_step(logprobsf,\n",
    "                                                unaug_logprobsf,\n",
    "                                                bdash,\n",
    "                                                t-divm,\n",
    "                                                beam_seq_table[divm],\n",
    "                                                beam_seq_logprobs_table[divm],\n",
    "                                                beam_logprobs_sum_table[divm],\n",
    "                                                state_table[divm])\n",
    "\n",
    "                    # if time's up... or if end token is reached then copy beams\n",
    "                    for vix in range(bdash):\n",
    "                        if beam_seq_table[divm][t-divm,vix] == 0 or t == max_length + divm - 1:\n",
    "                            final_beam = {\n",
    "                                'seq': beam_seq_table[divm][:, vix].clone(), \n",
    "                                'logps': beam_seq_logprobs_table[divm][:, vix].clone(),\n",
    "                                'unaug_p': beam_seq_logprobs_table[divm][:, vix].sum().item(),\n",
    "                                'p': beam_logprobs_sum_table[divm][vix].item()\n",
    "                            }\n",
    "                            if max_ppl:\n",
    "                                final_beam['p'] = final_beam['p'] / (t-divm+1)\n",
    "                            done_beams_table[divm].append(final_beam)\n",
    "                            # don't continue beams from finished sequences\n",
    "                            beam_logprobs_sum_table[divm][vix] = -1000\n",
    "\n",
    "                    # move the current group one step forward in time\n",
    "                    wt = beam_seq_table[divm][t-divm]\n",
    "                    kwargs['WT'] = wt.cuda()\n",
    "                    kwargs['STATE'] = state_table[divm]\n",
    "                    logprobs_table[divm], state_table[divm] = self.get_logprobs_state(**kwargs)\n",
    "\n",
    "        # all beams are sorted by their log-probabilities\n",
    "        done_beams_table = [sorted(done_beams_table[i], key=lambda x: -x['p'])[:bdash] for i in range(group_size)]\n",
    "        done_beams = reduce(lambda a,b:a+b, done_beams_table)\n",
    "        return done_beams\n",
    "\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1)\n",
    "    return subsequent_mask == 0\n",
    "\n",
    "\n",
    "def expand_tensor(tensor, size, dim=1):\n",
    "    if size == 1 or tensor is None:\n",
    "        return tensor\n",
    "    tensor = tensor.unsqueeze(dim)\n",
    "    tensor = tensor.expand(list(tensor.shape[:dim]) + [size] + list(tensor.shape[dim+1:])).contiguous()\n",
    "    tensor = tensor.view(list(tensor.shape[:dim-1]) + [-1] + list(tensor.shape[dim+1:]))\n",
    "    return tensor\n",
    "\n",
    "\n",
    "class PureT(BasicModel):\n",
    "    def __init__(self):\n",
    "        super(PureT, self).__init__()\n",
    "        self.vocab_size = vocab_size + 1\n",
    "        \n",
    "        self.backbone = SwinTransformer(\n",
    "            img_size=384, \n",
    "            embed_dim=192, \n",
    "            depths=[2, 2, 18, 2],\n",
    "            num_heads=[6, 12, 24, 48],\n",
    "            window_size=12,\n",
    "            num_classes=1000\n",
    "        )\n",
    "        print('load pretrained weights!')\n",
    "        self.backbone.load_weights(\n",
    "            './swin_large_patch4_window12_384_22kto1k_no_head.pth'\n",
    "        )\n",
    "        # Freeze parameters\n",
    "        for _name, _weight in self.backbone.named_parameters():\n",
    "            _weight.requires_grad = False\n",
    "            # print(_name, _weight.requires_grad)\n",
    "        \n",
    "        # raw Dimension to Model Dimension\n",
    "        if att_feats_dim == embedded_dim_size:\n",
    "            self.att_embed = nn.Identity()\n",
    "        else:\n",
    "            self.att_embed = nn.Sequential(\n",
    "                nn.Linear(att_feats_dim, embedded_dim_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.LayerNorm(embedded_dim_size) if False == True else nn.Identity(),\n",
    "                nn.Dropout(0.0)\n",
    "            )\n",
    "        \n",
    "        use_gx = True\n",
    "        self.encoder = Encoder(\n",
    "            embed_dim=embedded_dim_size, \n",
    "            input_resolution=(12, 12), \n",
    "            depth=encoder_decoder_layers, \n",
    "            num_heads=encoder_decoder_heads, \n",
    "            window_size=6,\n",
    "            shift_size=3,\n",
    "            mlp_ratio=4,\n",
    "            dropout=0.1,\n",
    "            use_gx = use_gx\n",
    "        )\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "            vocab_size = self.vocab_size, \n",
    "            embed_dim = embedded_dim_size, \n",
    "            depth = encoder_decoder_layers,\n",
    "            num_heads = encoder_decoder_heads, \n",
    "            dropout = 0.0, \n",
    "            ff_dropout = 0.1,\n",
    "            use_gx = use_gx\n",
    "        )\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        att_feats = kwargs['ATT_FEATS']\n",
    "        seq = kwargs['INPUT_SENT']\n",
    "        \n",
    "        # backbone forward\n",
    "        att_feats = self.backbone(att_feats)\n",
    "        \n",
    "        # att_mask for features\n",
    "        att_mask = kwargs['ATT_FEATS_MASK']\n",
    "        att_mask = expand_tensor(att_mask, 5)\n",
    "        att_feats = expand_tensor(att_feats, 5)\n",
    "\n",
    "        # words mask [B, L, L]\n",
    "        ##############################################\n",
    "        seq_mask = (seq > 0).type(torch.cuda.IntTensor)\n",
    "        seq_mask[:,0] += 1\n",
    "        seq_mask = seq_mask.unsqueeze(-2)\n",
    "        seq_mask = seq_mask & subsequent_mask(seq.size(-1)).to(seq_mask)\n",
    "        seq_mask = seq_mask.type(torch.cuda.FloatTensor)\n",
    "        ##############################################\n",
    "\n",
    "        att_feats = self.att_embed(att_feats)\n",
    "        gx, encoder_out = self.encoder(att_feats, att_mask)\n",
    "        decoder_out = self.decoder(gx, seq, encoder_out, seq_mask, att_mask)\n",
    "        return F.log_softmax(decoder_out, dim=-1)\n",
    "\n",
    "    def get_logprobs_state(self, **kwargs):\n",
    "        wt = kwargs['WT']\n",
    "        state = kwargs['STATE']\n",
    "        encoder_out = kwargs['ATT_FEATS']\n",
    "        \n",
    "        att_mask = kwargs['ATT_FEATS_MASK']\n",
    "        gx = kwargs['GV_FEAT']\n",
    "        # p_att_feats = kwargs[cfg.PARAM.P_ATT_FEATS]\n",
    "\n",
    "        # state[0][0]: [B, seq_len-1]，previously generated words\n",
    "        # ys: [B, seq_len]\n",
    "        if state is None:\n",
    "            ys = wt.unsqueeze(1)\n",
    "        else:\n",
    "            ys = torch.cat([state[0][0], wt.unsqueeze(1)], dim=1)\n",
    "            \n",
    "        seq_mask = subsequent_mask(ys.size(1)).to(encoder_out.device).type(torch.cuda.FloatTensor)[:, -1, :].unsqueeze(1)\n",
    "        \n",
    "        # [B, 1, Vocab_Size] --> [B, Vocab_Size]\n",
    "        decoder_out = self.decoder(gx, ys[:, -1].unsqueeze(-1), encoder_out, seq_mask, att_mask).squeeze(1)\n",
    "        \n",
    "        logprobs = F.log_softmax(decoder_out, dim=-1)\n",
    "        return logprobs, [ys.unsqueeze(0)]\n",
    "\n",
    "    def _expand_state(self, batch_size, beam_size, cur_beam_size, selected_beam):\n",
    "        def fn(s):\n",
    "            shape = [int(sh) for sh in s.shape]\n",
    "            beam = selected_beam\n",
    "            for _ in shape[1:]:\n",
    "                beam = beam.unsqueeze(-1)\n",
    "            s = torch.gather(s.view(*([batch_size, cur_beam_size] + shape[1:])), 1,\n",
    "                             beam.expand(*([batch_size, beam_size] + shape[1:])))\n",
    "            s = s.view(*([-1, ] + shape[1:]))\n",
    "            return s\n",
    "        return fn\n",
    "\n",
    "    # the beam search code is inspired by https://github.com/aimagelab/meshed-memory-transformer\n",
    "    def decode_beam(self, **kwargs):\n",
    "        att_feats = kwargs['ATT_FEATS']\n",
    "        att_mask = kwargs['ATT_FEATS_MASK']\n",
    "        beam_size = kwargs['BEAM_SIZE']\n",
    "        batch_size = att_feats.size(0)\n",
    "        seq_logprob = torch.zeros((batch_size, 1, 1)).cuda()\n",
    "        log_probs = []\n",
    "        selected_words = None\n",
    "        seq_mask = torch.ones((batch_size, beam_size, 1)).cuda()\n",
    "\n",
    "        att_feats = self.backbone(att_feats)\n",
    "        att_feats = self.att_embed(att_feats)\n",
    "        gx, encoder_out = self.encoder(att_feats, att_mask)\n",
    "        # p_att_feats = self.decoder.precompute(encoder_out)\n",
    "\n",
    "        state = None\n",
    "        wt = Variable(torch.zeros(batch_size, dtype=torch.long).cuda())\n",
    "        kwargs['ATT_FEATS'] = encoder_out\n",
    "        kwargs['GV_FEAT'] = gx\n",
    "        # kwargs[cfg.PARAM.P_ATT_FEATS] = p_att_feats\n",
    "\n",
    "        outputs = []\n",
    "        self.decoder.init_buffer(batch_size)\n",
    "        for t in range(max_length):\n",
    "            cur_beam_size = 1 if t == 0 else beam_size\n",
    "\n",
    "            kwargs['WT'] = wt\n",
    "            kwargs['STATE'] = state\n",
    "            word_logprob, state = self.get_logprobs_state(**kwargs)\n",
    "            # [B*cur_beam_size, Vocab_size] --> [B, cur_beam_size, Vocab_size]\n",
    "            word_logprob = word_logprob.view(batch_size, cur_beam_size, -1)\n",
    "            # sum of logprob\n",
    "            # [B, cur_beam_size, Vocab_size]\n",
    "            candidate_logprob = seq_logprob + word_logprob\n",
    "\n",
    "            # Mask sequence if it reaches EOS\n",
    "            if t > 0:\n",
    "                mask = (selected_words.view(batch_size, cur_beam_size) != 0).float().unsqueeze(-1)\n",
    "                seq_mask = seq_mask * mask\n",
    "                word_logprob = word_logprob * seq_mask.expand_as(word_logprob)\n",
    "                old_seq_logprob = seq_logprob.expand_as(candidate_logprob).contiguous()\n",
    "                old_seq_logprob[:, :, 1:] = -999\n",
    "                candidate_logprob = seq_mask * candidate_logprob + old_seq_logprob * (1 - seq_mask)\n",
    "\n",
    "            # [B, beam_size], [B, beam_size]\n",
    "            selected_idx, selected_logprob = self.select(batch_size, beam_size, t, candidate_logprob)\n",
    "            selected_beam = selected_idx // candidate_logprob.shape[-1]\n",
    "            selected_words = selected_idx - selected_beam * candidate_logprob.shape[-1]\n",
    "\n",
    "            # update buffer\n",
    "            self.decoder.apply_to_states(self._expand_state(batch_size, beam_size, cur_beam_size, selected_beam))\n",
    "            seq_logprob = selected_logprob.unsqueeze(-1)\n",
    "            seq_mask = torch.gather(seq_mask, 1, selected_beam.unsqueeze(-1))\n",
    "            outputs = list(torch.gather(o, 1, selected_beam.unsqueeze(-1)) for o in outputs)\n",
    "            outputs.append(selected_words.unsqueeze(-1))\n",
    "\n",
    "            this_word_logprob = torch.gather(word_logprob, 1,\n",
    "                selected_beam.unsqueeze(-1).expand(batch_size, beam_size, word_logprob.shape[-1]))\n",
    "            this_word_logprob = torch.gather(this_word_logprob, 2, selected_words.unsqueeze(-1))\n",
    "            log_probs = list(\n",
    "                torch.gather(o, 1, selected_beam.unsqueeze(-1).expand(batch_size, beam_size, 1)) for o in log_probs)\n",
    "            log_probs.append(this_word_logprob)\n",
    "            selected_words = selected_words.view(-1, 1)\n",
    "            wt = selected_words.squeeze(-1)\n",
    "\n",
    "            if t == 0:\n",
    "                # expand input\n",
    "                encoder_out = expand_tensor(encoder_out, beam_size)\n",
    "                gx = expand_tensor(gx, beam_size)\n",
    "                att_mask = expand_tensor(att_mask, beam_size)\n",
    "                state[0] = state[0].squeeze(0)\n",
    "                state[0] = expand_tensor(state[0], beam_size)\n",
    "                state[0] = state[0].unsqueeze(0)\n",
    "\n",
    "                # p_att_feats_tmp = []\n",
    "                # for p_feat in p_att_feats:\n",
    "                #     p_key, p_value2 = p_feat\n",
    "                #     p_key = utils.expand_tensor(p_key, beam_size)\n",
    "                #     p_value2 = utils.expand_tensor(p_value2, beam_size)\n",
    "                #     p_att_feats_tmp.append((p_key, p_value2))\n",
    "\n",
    "                kwargs['ATT_FEATS'] = encoder_out\n",
    "                kwargs['GV_FEAT'] = gx\n",
    "                kwargs['ATT_FEATS_MASK'] = att_mask\n",
    "                # kwargs[cfg.PARAM.P_ATT_FEATS] = p_att_feats_tmp\n",
    " \n",
    "        seq_logprob, sort_idxs = torch.sort(seq_logprob, 1, descending=True)\n",
    "        outputs = torch.cat(outputs, -1)\n",
    "        outputs = torch.gather(outputs, 1, sort_idxs.expand(batch_size, beam_size, max_length))\n",
    "        log_probs = torch.cat(log_probs, -1)\n",
    "        log_probs = torch.gather(log_probs, 1, sort_idxs.expand(batch_size, beam_size, max_length))\n",
    "\n",
    "        outputs = outputs.contiguous()[:, 0]\n",
    "        log_probs = log_probs.contiguous()[:, 0]\n",
    "\n",
    "        self.decoder.clear_buffer()\n",
    "        return outputs, log_probs\n",
    "\n",
    "    def decode(self, **kwargs):\n",
    "        beam_size = kwargs['BEAM_SIZE']\n",
    "        greedy_decode = kwargs['GREEDY_DECODE']\n",
    "        att_feats = kwargs['ATT_FEATS']\n",
    "        att_mask = kwargs['ATT_FEATS_MASK']\n",
    "\n",
    "        batch_size = att_feats.size(0)\n",
    "        att_feats = self.backbone(att_feats)\n",
    "        att_feats = self.att_embed(att_feats)\n",
    "        gx, encoder_out = self.encoder(att_feats, att_mask)\n",
    "        # p_att_feats = self.decoder.precompute(encoder_out)\n",
    "        self.decoder.init_buffer(batch_size)\n",
    "        \n",
    "        state = None\n",
    "        sents = Variable(torch.zeros((batch_size, max_length), dtype=torch.long).cuda())\n",
    "        logprobs = Variable(torch.zeros(batch_size, max_length).cuda())\n",
    "        wt = Variable(torch.zeros(batch_size, dtype=torch.long).cuda())\n",
    "        unfinished = wt.eq(wt)\n",
    "        kwargs['ATT_FEATS'] = encoder_out\n",
    "        kwargs['GV_FEAT'] = gx\n",
    "        # kwargs[cfg.PARAM.P_ATT_FEATS] = p_att_feats\n",
    "        \n",
    "        # inference word by word\n",
    "        for t in range(max_length):\n",
    "            kwargs['WT'] = wt\n",
    "            kwargs['STATE'] = state\n",
    "            logprobs_t, state = self.get_logprobs_state(**kwargs)\n",
    "            \n",
    "            if greedy_decode:\n",
    "                logP_t, wt = torch.max(logprobs_t, 1)\n",
    "            else:\n",
    "                probs_t = torch.exp(logprobs_t)\n",
    "                wt = torch.multinomial(probs_t, 1)\n",
    "                logP_t = logprobs_t.gather(1, wt)\n",
    "            wt = wt.view(-1).long()\n",
    "            unfinished = unfinished * (wt > 0)\n",
    "            wt = wt * unfinished.type_as(wt)\n",
    "            sents[:,t] = wt\n",
    "            logprobs[:,t] = logP_t.view(-1)\n",
    "\n",
    "            if unfinished.sum() == 0:\n",
    "                break\n",
    "        self.decoder.clear_buffer()\n",
    "        return sents, logprobs\n",
    "\n",
    "# torch.Size([32, 3, 224, 224])                                                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "Trains the model and saves the best (lowest val error) and last model\n",
    "\n",
    "This section does the following actions:\n",
    "1. Creates the PureT model\n",
    "2. Sets up optimizer, scheduler, counter for training\n",
    "3. Trains for num_epochs epochs\n",
    "2. Each Epoch has valadation accuracy calculated TODO\n",
    "3. Save the model with the best valadation accuracy TODO\n",
    "4. Save the model when the max number of epochs has been reached TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained weights!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to create tensor with negative dimension -1: [-1, -1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 17\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Model setup\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Not sure where vocab_size (maybe from the gpt-2 tokenizer?) or embed_dim come from yet\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# But num_heads and window_size are from Table 6 of the paper https://arxiv.org/pdf/2203.15350\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPureT\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Setup for training\u001b[39;00m\n\u001b[0;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n",
      "Cell \u001b[1;32mIn[28], line 1501\u001b[0m, in \u001b[0;36mPureT.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1488\u001b[0m use_gx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m Encoder(\n\u001b[0;32m   1490\u001b[0m     embed_dim\u001b[38;5;241m=\u001b[39membedded_dim_size, \n\u001b[0;32m   1491\u001b[0m     input_resolution\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m12\u001b[39m), \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     use_gx \u001b[38;5;241m=\u001b[39m use_gx\n\u001b[0;32m   1499\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoder_decoder_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoder_decoder_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mff_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse_gx\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 1220\u001b[0m, in \u001b[0;36mDecoder.__init__\u001b[1;34m(self, vocab_size, embed_dim, depth, num_heads, dropout, ff_dropout, use_gx)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gx \u001b[38;5;241m=\u001b[39m use_gx\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth):\n\u001b[1;32m-> 1220\u001b[0m     sublayer \u001b[38;5;241m=\u001b[39m \u001b[43mDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mff_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mff_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse_gx\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(sublayer)\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 1102\u001b[0m, in \u001b[0;36mDecoderLayer.__init__\u001b[1;34m(self, embed_dim, num_heads, dropout, ff_dropout, use_gx)\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, ff_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, use_gx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28msuper\u001b[39m(DecoderLayer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_attn \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(embed_dim)\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_att \u001b[38;5;241m=\u001b[39m MultiHeadSelfAttention(\n\u001b[0;32m   1109\u001b[0m         embed_dim \u001b[38;5;241m=\u001b[39m embed_dim, \n\u001b[0;32m   1110\u001b[0m         num_heads \u001b[38;5;241m=\u001b[39m num_heads\n\u001b[0;32m   1111\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[28], line 1039\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.__init__\u001b[1;34m(self, embed_dim, num_heads)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m-> 1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_linear \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_linear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, embed_dim)\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_linear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, embed_dim)\n",
      "File \u001b[1;32mc:\\Programming\\Python3-12-3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to create tensor with negative dimension -1: [-1, -1]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Model setup\n",
    "# Not sure where vocab_size (maybe from the gpt-2 tokenizer?) or embed_dim come from yet\n",
    "# But num_heads and window_size are from Table 6 of the paper https://arxiv.org/pdf/2203.15350\n",
    "model = PureT()\n",
    "\n",
    "# Setup for training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=patience)\n",
    "\n",
    "# Values to remember training performance\n",
    "stop_counter = 0\n",
    "train_losses = []\n",
    "best_val_loss = float('inf')\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Epoch setup\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Loop through data loader batches\n",
    "    train_dataloader_iter = tqdm(train_dataset_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "    for i, data in enumerate(train_dataloader_iter):\n",
    "        \n",
    "        # Get values from data loader\n",
    "        pixel_vals = data[\"pixel_values\"].to(device)\n",
    "        captions = data[\"labels\"].to(device)\n",
    "\n",
    "        # Generate outputs\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images=pixel_vals, captions=captions)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Grad descent\n",
    "        loss.backwards()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Training Metrics\n",
    "TODO ALL\n",
    "Evalutes the best model on BLEU, ROUGE, and SPICE\n",
    "\n",
    "This section does the following actions:\n",
    "1. Loads the model with the highest valadation accuracy\n",
    "2. Predict all captions with best model\n",
    "3. Calculates ROUGE score\n",
    "4. Calculates BLEU score\n",
    "5. Calculates SPICE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Predict captions\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(images\u001b[38;5;241m=\u001b[39mpixel_vals, captions\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Format labels\u001b[39;00m\n\u001b[0;32m     18\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;66;03m#not sure about logits\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "# Run through valadation set with best model\n",
    "predictions = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "       for data in valid_dataset_loader:\n",
    "\n",
    "              # get data from batch\n",
    "              pixel_vals = data[\"pixel_values\"].to(device)\n",
    "              labels = data[\"labels\"].to(device)\n",
    "       \n",
    "              # Predict captions\n",
    "              outputs = model(images=pixel_vals, captions=labels)\n",
    "\n",
    "              # Format labels\n",
    "              logits = outputs.logits.detach().cpu() #not sure about logits\n",
    "              predictions.extend(logits.argmax(dim=-1).tolist())\n",
    "              labels.extend(labels.tolist())\n",
    "    \n",
    "\n",
    "# Format predictions into Hugging Face class\n",
    "eval_predictions = EvalPrediction(predictions=predictions, label_ids=labels)\n",
    "\n",
    "predictions = eval_predictions.predictions\n",
    "labels = eval_predictions.label_ids\n",
    "\n",
    "# Tokenize predictions and reference captions\n",
    "predictions_str = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "labels_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Load test evaluators\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Compute and print Rouge-1, Rogue-2, RougeL\n",
    "rouge_result = rouge.compute(predictions=predictions_str, references=labels_str)\n",
    "rouge_result = {k: round(v * 100, 4) for k, v in rouge_result.items()}\n",
    "print (\"ROUGE Metrics: \\nROUGE-1: \" + rouge_result.get(\"rouge1\", 0) + \n",
    "       \"\\nROUGE-2: \" + rouge_result.get(\"rouge2\", 0) + \n",
    "       \"\\nROUGE-L: \" + rouge_result.get(\"rougeL\", 0))\n",
    "\n",
    "\n",
    "# Compute and print BLEU metrics\n",
    "bleu_result = bleu.compute(predictions=predictions_str, references=labels_str)\n",
    "bleu_score = round(bleu_result[\"bleu\"] * 100, 4)\n",
    "print (\"BLEU Metrics: \" + bleu_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
