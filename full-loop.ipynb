{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Running:\n",
    "Please Install all from the requirements.txt (pip install -r requirements.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_length = 32\n",
    "coco_dataset_ratio = 50\n",
    "coco_dataset_dir = \"./coco\"\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "patience = 3\n",
    "weight_decay = 1e-5\n",
    "encoder_model = \"microsoft/swin-base-patch4-window7-224-in22k\"\n",
    "decoder_model = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Format datasets\n",
    "This will take some time to finishing running the first time. It took me roughly 40 minutes.\n",
    "\n",
    "This section does the following actions:\n",
    "1. Downloads the Dataset\n",
    "2. Keeps images with only 3 or 4 dim\n",
    "3. Transforms the dataset \n",
    "4. Turns the data set into data loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programming\\Python3-12-3\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for HuggingFaceM4/COCO contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/COCO\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTImageProcessor, GPT2TokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Download the train, val and test splits of the COCO dataset\n",
    "train_ds = load_dataset(\"HuggingFaceM4/COCO\", split=f\"train[:{coco_dataset_ratio}%]\", cache_dir=coco_dataset_dir)\n",
    "valid_ds = load_dataset(\"HuggingFaceM4/COCO\", split=f\"validation[:{coco_dataset_ratio}%]\", cache_dir=coco_dataset_dir)\n",
    "test_ds = load_dataset(\"HuggingFaceM4/COCO\", split=\"test\", cache_dir=coco_dataset_dir)\n",
    "\n",
    "\n",
    "# Filter all non 3 or 4 dim images out\n",
    "# Can change num_proc, but might be errors with np\n",
    "train_ds = train_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=1)\n",
    "valid_ds = valid_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=1)\n",
    "test_ds = test_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=1)\n",
    "\n",
    "\n",
    "# Does pre processing on the data set\n",
    "# This includes pre-trained ViTimage feature extraction and tokenizing captions\n",
    "# I am unsure if the paper does any of this pre processing\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(decoder_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "image_processor = ViTImageProcessor.from_pretrained(encoder_model)\n",
    "\n",
    "def preprocess(items):\n",
    "    pixel_values = image_processor(items[\"image\"], return_tensors=\"pt\").pixel_values.to(device)\n",
    "    targets = tokenizer([sentence[\"raw\"] for sentence in items[\"sentences\"]],\n",
    "                        max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
    "    return {'pixel_values': pixel_values, 'labels': targets[\"input_ids\"]}\n",
    "\n",
    "train_dataset = train_ds.with_transform(preprocess)\n",
    "valid_dataset = valid_ds.with_transform(preprocess)\n",
    "test_dataset = test_ds.with_transform(preprocess)\n",
    "\n",
    "\n",
    "# Turns the dataset into a torch DataLoader\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.stack([x['labels'] for x in batch])\n",
    "    }\n",
    "\n",
    "train_dataset_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset_loader = DataLoader(valid_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)\n",
    "test_dataset_loader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "Creates the PureT model from the paper\n",
    "\n",
    "This section does the following actions:\n",
    "1. Defines the PureT model from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, einsum\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "class PureT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, window_size):\n",
    "        super(PureT, self).__init__()\n",
    "\n",
    "        # Swin Transformer\n",
    "        self.swin_t = torchvision.models.swin_b()\n",
    "\n",
    "        # Before Refining Encoder (bre)\n",
    "        #self.bre_linear = nn.Linear() #[[add dime, just 512]]\n",
    "        #self.bre_avg_pool = nn.AvgPool1d() #[[change dim?]]\n",
    "\n",
    "        # Refining Encoder TODO\n",
    "        self.refine_encoder = nn.Sequential(\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        print (images.shape)\n",
    "        swin_output = self.swin_t(images)\n",
    "        print (swin_output.shape)\n",
    "\n",
    "        #TODO\n",
    "\n",
    "\n",
    "        #return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "Trains the model and saves the best (lowest val error) and last model\n",
    "\n",
    "This section does the following actions:\n",
    "1. Creates the PureT model\n",
    "2. Sets up optimizer, scheduler, counter for training\n",
    "3. Trains for num_epochs epochs\n",
    "2. Each Epoch has valadation accuracy calculated TODO\n",
    "3. Save the model with the best valadation accuracy TODO\n",
    "4. Save the model when the max number of epochs has been reached TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/8835 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     46\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images\u001b[38;5;241m=\u001b[39mpixel_vals, captions\u001b[38;5;241m=\u001b[39mcaptions)\n\u001b[1;32m---> 47\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Grad descent\u001b[39;00m\n\u001b[0;32m     50\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackwards()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Model setup\n",
    "# Not sure where vocab_size (maybe from the gpt-2 tokenizer?) or embed_dim come from yet\n",
    "# But num_heads and window_size are from Table 6 of the paper https://arxiv.org/pdf/2203.15350\n",
    "model = PureT(vocab_size=30522, embed_dim=768, num_heads=3, window_size=12)\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup for training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=patience)\n",
    "\n",
    "# Values to remember training performance\n",
    "stop_counter = 0\n",
    "train_losses = []\n",
    "best_val_loss = float('inf')\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Epoch setup\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Loop through data loader batches\n",
    "    train_dataloader_iter = tqdm(train_dataset_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "    for i, data in enumerate(train_dataloader_iter):\n",
    "        \n",
    "        # Get values from data loader\n",
    "        pixel_vals = data[\"pixel_values\"].to(device)\n",
    "        captions = data[\"labels\"].to(device)\n",
    "\n",
    "        # Generate outputs\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images=pixel_vals, captions=captions)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Grad descent\n",
    "        loss.backwards()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Training Metrics\n",
    "TODO ALL\n",
    "Evalutes the best model on BLEU, ROUGE, and SPICE\n",
    "\n",
    "This section does the following actions:\n",
    "1. Loads the model with the highest valadation accuracy\n",
    "2. Predict all captions with best model\n",
    "3. Calculates ROUGE score\n",
    "4. Calculates BLEU score\n",
    "5. Calculates SPICE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 1000])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images\u001b[38;5;241m=\u001b[39mpixel_vals, captions\u001b[38;5;241m=\u001b[39mcaptions)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Format labels\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m     19\u001b[0m predictions\u001b[38;5;241m.\u001b[39mextend(logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     20\u001b[0m labels\u001b[38;5;241m.\u001b[39mextend(captions\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "# Run through valadation set with best model\n",
    "predictions = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "       for data in valid_dataset_loader:\n",
    "\n",
    "              # get data from batch\n",
    "              pixel_vals = data[\"pixel_values\"].to(device)\n",
    "              labels = data[\"labels\"].to(device)\n",
    "       \n",
    "              # Predict captions\n",
    "              outputs = model(images=pixel_vals, captions=labels)\n",
    "\n",
    "              # Format labels\n",
    "              logits = outputs.logits.detach().cpu() #not sure about logits\n",
    "              predictions.extend(logits.argmax(dim=-1).tolist())\n",
    "              labels.extend(labels.tolist())\n",
    "    \n",
    "\n",
    "# Format predictions into Hugging Face class\n",
    "eval_predictions = EvalPrediction(predictions=predictions, label_ids=labels)\n",
    "\n",
    "predictions = eval_predictions.predictions\n",
    "labels = eval_predictions.label_ids\n",
    "\n",
    "# Tokenize predictions and reference captions\n",
    "predictions_str = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "labels_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Load test evaluators\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Compute and print Rouge-1, Rogue-2, RougeL\n",
    "rouge_result = rouge.compute(predictions=predictions_str, references=labels_str)\n",
    "rouge_result = {k: round(v * 100, 4) for k, v in rouge_result.items()}\n",
    "print (\"ROUGE Metrics: \\nROUGE-1: \" + rouge_result.get(\"rouge1\", 0) + \n",
    "       \"\\nROUGE-2: \" + rouge_result.get(\"rouge2\", 0) + \n",
    "       \"\\nROUGE-L: \" + rouge_result.get(\"rougeL\", 0))\n",
    "\n",
    "\n",
    "# Compute and print BLEU metrics\n",
    "bleu_result = bleu.compute(predictions=predictions_str, references=labels_str)\n",
    "bleu_score = round(bleu_result[\"bleu\"] * 100, 4)\n",
    "print (\"BLEU Metrics: \" + bleu_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
