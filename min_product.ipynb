{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Running:\n",
    "Please Install all from the requirements.txt (pip install -r requirements.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder_decoder_layers = 3\n",
    "encoder_decoder_heads = 8\n",
    "embedded_dim = 768 # Don't change\n",
    "max_length = 32 \n",
    "coco_dataset_ratio = 50\n",
    "coco_dataset_dir = \"./coco\"\n",
    "vocab_size = 50257 # Don't change\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4 # was 1e-3, not the problem\n",
    "patience = 3\n",
    "weight_decay = 1e-5 # was 1e-5, not the problem\n",
    "preprocess_swin_model = \"microsoft/swin-tiny-patch4-window7-224\"\n",
    "encoder_model = \"microsoft/swin-base-patch4-window7-224-in22k\"\n",
    "decoder_model = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Format datasets\n",
    "This will take some time to finishing running the first time. It took me roughly 40 minutes.\n",
    "\n",
    "This section does the following actions:\n",
    "1. Downloads the Dataset\n",
    "2. Keeps images with only 3 or 4 dim\n",
    "3. Transforms the dataset \n",
    "4. Turns the data set into data loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programming\\Python3-12-3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Programming\\Python3-12-3\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for HuggingFaceM4/COCO contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/COCO\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTImageProcessor, GPT2TokenizerFast, AutoImageProcessor, SwinModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Download the train, val and test splits of the COCO dataset\n",
    "train_ds = load_dataset(\"HuggingFaceM4/COCO\", split=f\"train[:{coco_dataset_ratio}%]\", cache_dir=coco_dataset_dir)\n",
    "valid_ds = load_dataset(\"HuggingFaceM4/COCO\", split=f\"validation[:{coco_dataset_ratio}%]\", cache_dir=coco_dataset_dir)\n",
    "test_ds = load_dataset(\"HuggingFaceM4/COCO\", split=\"test\", cache_dir=coco_dataset_dir)\n",
    "\n",
    "# Filter all non 3 or 4 dim images out\n",
    "# Can change num_proc, but might be errors with np\n",
    "train_ds = train_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=1)\n",
    "valid_ds = valid_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=1)\n",
    "test_ds = test_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=1)\n",
    "\n",
    "# Does pre processing on the data set\n",
    "# This includes pre-trained ViTimage feature extraction and tokenizing captions\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(decoder_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "image_processor = ViTImageProcessor.from_pretrained(encoder_model)\n",
    "image_processor_swin = SwinModel.from_pretrained(preprocess_swin_model).to(device)\n",
    "\n",
    "def preprocess(items):\n",
    "    # Image pre-processing\n",
    "    # use ViT and SWIN since no back prop\n",
    "    pixel_values = image_processor(items[\"image\"], return_tensors=\"pt\").pixel_values.to(device)\n",
    "    with torch.no_grad():\n",
    "        pixel_values = image_processor_swin (pixel_values).last_hidden_state\n",
    "    pixel_values = pixel_values.to('cpu')\n",
    "\n",
    "    # tokenize\n",
    "    targets = tokenizer(items[\"sentences\"]['raw'],\n",
    "                        max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Keep image file for easy examples later\n",
    "    img_file = items['filepath']\n",
    "    return {'pixel_values': pixel_values, 'labels': targets[\"input_ids\"], 'image_file': img_file}\n",
    "\n",
    "\n",
    "TRAIN_SET_PATH = './preprocessed/train_set.pt'\n",
    "TEST_SET_PATH = './preprocessed/val_set.pt'\n",
    "VALID_SET_PATH = './preprocessed/test_set.pt'\n",
    "\n",
    "# Pre process train dataset if it doesn't exist\n",
    "train_dataset = None\n",
    "if os.path.isfile(TRAIN_SET_PATH):\n",
    "    train_dataset = torch.load(TRAIN_SET_PATH)\n",
    "\n",
    "else:\n",
    "    train_dataset = train_ds.map(preprocess)\n",
    "    torch.save(train_dataset, TRAIN_SET_PATH)\n",
    "    \n",
    "# Pre process val dataset if it doesn't exist\n",
    "valid_dataset = None\n",
    "if os.path.isfile(VALID_SET_PATH):\n",
    "    valid_dataset = torch.load(VALID_SET_PATH)\n",
    "\n",
    "else:\n",
    "    valid_dataset = valid_ds.map(preprocess)\n",
    "    torch.save(valid_dataset, VALID_SET_PATH)\n",
    "\n",
    "# Pre process test dataset if it doesn't exist\n",
    "test_dataset = None\n",
    "if os.path.isfile(TEST_SET_PATH):\n",
    "    test_dataset = torch.load(TEST_SET_PATH)\n",
    "\n",
    "else:\n",
    "    test_dataset = test_ds.map(preprocess)\n",
    "    torch.save(test_dataset, TEST_SET_PATH)\n",
    "\n",
    "\n",
    "\n",
    "# Turns the dataset into a torch DataLoader\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([torch.tensor(x['pixel_values']) for x in batch]),\n",
    "        'labels': torch.stack([torch.tensor(x['labels']) for x in batch]),\n",
    "        'image_file': [x[\"image_file\"] for x in batch]\n",
    "    }\n",
    "\n",
    "train_dataset_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset_loader = DataLoader(valid_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)\n",
    "test_dataset_loader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "Creates the PureT model from the paper\n",
    "\n",
    "This section does the following actions:\n",
    "1. Creates the SWIN Transformer used by PureT\n",
    "2. Creates the PureT encoder\n",
    "3. Creates the PureT decoder\n",
    "4. Creates the PureT model\n",
    "\n",
    "Download the pre-trained SwinT weights from here https://drive.google.com/drive/folders/1HBw5NGGw8DjkyNurksCP5v8a5f0FG7zU and put them in this folder before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from transformers import SwinModel\n",
    "\n",
    "class BasicCaptionTransformer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCaptionTransformer, self).__init__()\n",
    "\n",
    "        # Image processing pre-done with ViTImageProcessor and Swin in the Downloading and Format datasets step\n",
    "        #self.swin = SwinModel.from_pretrained(preprocess_swin_model)\n",
    "\n",
    "        # build encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedded_dim, nhead=encoder_decoder_heads, batch_first=True)\n",
    "        self.encoders = nn.TransformerEncoder(encoder_layer, encoder_decoder_layers)\n",
    "\n",
    "        # embeddings\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedded_dim)\n",
    "\n",
    "        # build decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embedded_dim, nhead=encoder_decoder_heads, batch_first=True)\n",
    "        self.decoders = nn.TransformerDecoder(decoder_layer, encoder_decoder_layers)\n",
    "\n",
    "        # final\n",
    "        self.linear = nn.Linear(in_features=768, out_features=vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        x = images\n",
    "        #with torch.no_grad():\n",
    "        #    x = self.swin(x)\n",
    "        x = self.encoders(x)\n",
    "        \n",
    "        captions = self.embeddings(captions)\n",
    "        x = self.decoders(tgt=captions.to(x.dtype), memory=x)\n",
    "        return self.linear(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "Trains the model and saves the best (lowest val error) and last model\n",
    "\n",
    "This section does the following actions:\n",
    "1. Creates the Basic transformer model\n",
    "2. Sets up optimizer, scheduler, counter for training\n",
    "3. Trains for num_epochs epochs\n",
    "2. Each Epoch has valadation accuracy calculated\n",
    "3. Save the model with the best valadation accuracy TODO\n",
    "4. Save the model when the max number of epochs has been reached TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/4418 [00:00<?, ?it/s]c:\\Programming\\Python3-12-3\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Epoch 1/10:   2%|▏         | 100/4418 [02:11<1:31:11,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss so far is: 3.5211477351911142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   5%|▍         | 200/4418 [04:16<1:27:34,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss so far is: 3.291331408610895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   7%|▋         | 300/4418 [06:19<1:24:05,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss so far is: 3.2253024968813895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   9%|▉         | 400/4418 [08:22<1:21:42,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss so far is: 3.1825974674750688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  11%|█▏        | 500/4418 [10:33<1:34:15,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss so far is: 3.1494571727836775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  14%|█▎        | 600/4418 [12:56<1:25:16,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss so far is: 3.1233302512033556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  16%|█▌        | 700/4418 [15:20<1:26:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss so far is: 3.1015545070086086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# save loss\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m99\u001b[39m:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss so far is: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m (train_loss \u001b[38;5;241m/\u001b[39m i))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Loop setup\n",
    "model = BasicCaptionTransformer()\n",
    "model = model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# Train\n",
    "predictions_per_batch = batch_size * max_length\n",
    "stop_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_len = len(train_dataset_loader)\n",
    "val_len = len(valid_dataset_loader)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Loop through training data loader batches\n",
    "    train_loss = 0.0\n",
    "    train_dataloader_iter = tqdm(train_dataset_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "    for i, data in enumerate(train_dataloader_iter):\n",
    "        \n",
    "        # Get values from data loader\n",
    "        pixel_vals = data[\"pixel_values\"].squeeze(1).to(device)\n",
    "        captions = data[\"labels\"].squeeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images=pixel_vals, captions=captions)\n",
    "\n",
    "        loss = loss_function(outputs.permute(0,2,1), captions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # save loss\n",
    "        train_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print (\"Loss so far is: \" + str (train_loss / i))\n",
    "            #print (\"tensor: \", outputs)\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    valid_dataset_iter = tqdm(valid_dataset_loader,  desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valid_dataset_iter):\n",
    "            \n",
    "            # Get values from data loader\n",
    "            pixel_vals = data[\"pixel_values\"].squeeze(1).to(device)\n",
    "            captions = data[\"labels\"].squeeze(1).to(device)\n",
    "\n",
    "            outputs = model(images=pixel_vals, captions=captions)\n",
    "            loss = loss_function(outputs.permute(0,2,1), captions)\n",
    "\n",
    "            # save loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f'./models/model_epoch_{epoch+1}.pt')\n",
    "\n",
    "    train_losses.append(train_loss / train_len)\n",
    "    val_losses.append(val_loss / val_len)\n",
    "\n",
    "    # Print losses\n",
    "    # divide by length of data loader... \n",
    "    print(\"\\nEpoch: \" + str(epoch) + \n",
    "        \"\\nTrain Loss: \" + str(train_loss / train_len) +\n",
    "        \"\\nVal Loss: \" + str(val_loss / val_len))\n",
    "\n",
    "print (train_losses)\n",
    "print (val_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Training Metrics\n",
    "Computes the test loss and common test metrics\n",
    "\n",
    "This section does the following actions:\n",
    "1. Loads the specified model\n",
    "2. Runs through the test set and reports loss\n",
    "3. Runs through the val set for BLEU and ROUGE metrics\n",
    "4. Gives some images titles and saves them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (131980854.py, line 98)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 98\u001b[1;36m\u001b[0m\n\u001b[1;33m    model_input =  test_dataset[i][]\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from transformers import EvalPrediction\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load model\n",
    "MODEL_PATH = \"./models/model_epoch_8.pt\"\n",
    "LOAD_MODEL = True\n",
    "\n",
    "eval_model = None\n",
    "if LOAD_MODEL:\n",
    "       if \"model\" in locals():\n",
    "              model.to('cpu')\n",
    "\n",
    "       eval_model = BasicCaptionTransformer()\n",
    "       eval_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "       eval_model = eval_model.to(device)\n",
    "\n",
    "else:\n",
    "       eval_model = model\n",
    "\n",
    "# Eval setup\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "test_loss = 0.0\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "# Run through the test for test loss\n",
    "with torch.no_grad():\n",
    "       test_dataset_iter = tqdm(test_dataset_loader,  desc=f'Test Set Progress: ', leave=False)\n",
    "       for data in test_dataset_iter:\n",
    "\n",
    "              # get data from batch\n",
    "              pixel_vals = data[\"pixel_values\"].squeeze(1).to(device)\n",
    "              labels = data[\"labels\"].squeeze(1).to(device)\n",
    "\n",
    "              # Predict captions\n",
    "              outputs = eval_model(images=pixel_vals, captions=labels)\n",
    "              test_loss += loss_function(outputs.permute(0,2,1), labels)\n",
    "              print (test_loss)\n",
    "\n",
    "print (\"Test Loss: \" + str(test_loss / len(test_dataset_loader)))\n",
    "\n",
    "\n",
    "# Run through valadation set with best model\n",
    "predictions = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "       valid_dataset_iter = tqdm(valid_dataset_loader,  desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "       for data in valid_dataset_iter:\n",
    "\n",
    "              # get data from batch\n",
    "              pixel_vals = data[\"pixel_values\"].squeeze(1).to(device)\n",
    "              labels = data[\"labels\"].squeeze(1).to(device)\n",
    "       \n",
    "              # Predict captions\n",
    "              outputs = eval_model(images=pixel_vals, captions=labels)\n",
    "\n",
    "              # Format labels\n",
    "              logits = outputs.detach().cpu()\n",
    "              predictions.extend(logits.argmax(dim=-1).tolist())\n",
    "              labels.extend(labels.tolist())\n",
    "    \n",
    "\n",
    "# Format predictions into Hugging Face class\n",
    "eval_predictions = EvalPrediction(predictions=predictions, label_ids=labels)\n",
    "\n",
    "predictions = eval_predictions.predictions\n",
    "labels = eval_predictions.label_ids\n",
    "\n",
    "# Tokenize predictions and reference captions\n",
    "predictions_str = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "labels_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Load test evaluators\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Compute and print Rouge-1, Rogue-2, RougeL\n",
    "rouge_result = rouge.compute(predictions=predictions_str, references=labels_str)\n",
    "rouge_result = {k: round(v * 100, 4) for k, v in rouge_result.items()}\n",
    "print (\"ROUGE Metrics: \\nROUGE-1: \" + rouge_result.get(\"rouge1\", 0) + \n",
    "       \"\\nROUGE-2: \" + rouge_result.get(\"rouge2\", 0) + \n",
    "       \"\\nROUGE-L: \" + rouge_result.get(\"rougeL\", 0))\n",
    "\n",
    "\n",
    "# Compute and print BLEU metrics\n",
    "bleu_result = bleu.compute(predictions=predictions_str, references=labels_str)\n",
    "bleu_score = round(bleu_result[\"bleu\"] * 100, 4)\n",
    "print (\"BLEU Metrics: \" + bleu_score)\n",
    "\n",
    "\n",
    "# Get first 16 images and give them captiosn\n",
    "for i in range(16):\n",
    "       file_path = test_dataset[i][\"image_file\"]\n",
    "       model_input =  test_dataset[i][\"pixel_values\"]\n",
    "\n",
    "       # Make caption just beginning of sentence token (50256)\n",
    "       # It's also the padding token, maybe an issue?\n",
    "       sos_caption = torch.tensor([1, 2, 3])\n",
    "       output = model(model_input, sos_caption)\n",
    "\n",
    "\n",
    "       fig, ax = plt.subplot_mosaic([\n",
    "       ['hopper', 'mri']\n",
    "       ], figsize=(7, 3.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
